{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11631688,"sourceType":"datasetVersion","datasetId":6775341}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # 1. ðŸ“¦ INSTALLS & IMPORTS\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:53:04.988468Z\",\"iopub.execute_input\":\"2025-05-09T21:53:04.988790Z\",\"iopub.status.idle\":\"2025-05-09T21:53:08.413253Z\",\"shell.execute_reply.started\":\"2025-05-09T21:53:04.988768Z\",\"shell.execute_reply\":\"2025-05-09T21:53:08.412169Z\"},\"jupyter\":{\"outputs_hidden\":false}}\npip install neurokit2\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:53:08.414928Z\",\"iopub.execute_input\":\"2025-05-09T21:53:08.415205Z\",\"iopub.status.idle\":\"2025-05-09T21:53:11.765756Z\",\"shell.execute_reply.started\":\"2025-05-09T21:53:08.415181Z\",\"shell.execute_reply\":\"2025-05-09T21:53:11.764774Z\"},\"jupyter\":{\"outputs_hidden\":false}}\npip install entropy\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:53:11.767944Z\",\"iopub.execute_input\":\"2025-05-09T21:53:11.768232Z\",\"iopub.status.idle\":\"2025-05-09T21:53:11.773937Z\",\"shell.execute_reply.started\":\"2025-05-09T21:53:11.768207Z\",\"shell.execute_reply\":\"2025-05-09T21:53:11.773183Z\"}}\n# Standard libraries\nimport os\nfrom collections import Counter\n\n# Data processing and analysis\nimport numpy as np\nimport pandas as pd\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ECG processing\nimport neurokit2 as nk\n\n# Progress tracking\nfrom tqdm import tqdm\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom imblearn.over_sampling import SMOTE\n\n# Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n# Metrics\nfrom sklearn.metrics import (accuracy_score, classification_report, \n                           confusion_matrix, ConfusionMatrixDisplay,\n                           roc_auc_score, roc_curve, auc)\n\n# Serialization\nimport pickle\n\n# %% [markdown]\n# # 2. WESAD Dataset Loader and ECG Preprocessor\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:53:11.775126Z\",\"iopub.execute_input\":\"2025-05-09T21:53:11.775433Z\",\"iopub.status.idle\":\"2025-05-09T21:55:05.823901Z\",\"shell.execute_reply.started\":\"2025-05-09T21:53:11.775405Z\",\"shell.execute_reply\":\"2025-05-09T21:55:05.822835Z\"}}\n\n\nbase_path = \"/kaggle/input/wesad-full-dataset/WESAD/\"\nsubject_ids = [f\"S{i}\" for i in range(2, 18)]  # S2 to S17\nall_subjects_dfs = []\n\nfor subject_id in subject_ids:\n    txt_file = f\"{base_path}{subject_id}/{subject_id}_respiban.txt\"\n    pkl_file = f\"{base_path}{subject_id}/{subject_id}.pkl\"\n    \n    if not os.path.exists(txt_file) or not os.path.exists(pkl_file):\n        print(f\"Skipping {subject_id} - files not found\")\n        continue\n    \n    try:\n        # Load ECG\n        ecg_data = np.loadtxt(txt_file)\n        ecg_raw = ecg_data[:, 2]  # Column 2 = ECG\n        \n        # Load labels\n        with open(pkl_file, \"rb\") as f:\n            labels = pickle.load(f, encoding=\"latin1\")[\"label\"]\n        \n        # Ensure equal lengths\n        min_length = min(len(ecg_raw), len(labels))\n        ecg_raw = ecg_raw[:min_length]\n        labels = labels[:min_length]\n        \n        # Convert to mV\n        ecg_mv = ((ecg_raw / 65536) - 0.5) * 3.0\n        \n        # Create DataFrame\n        df = pd.DataFrame({\n            \"timestamp\": np.arange(min_length) / 700,\n            \"ecg_mv\": ecg_mv,\n            \"label\": labels,\n            \"subject_id\": subject_id\n        })\n        all_subjects_dfs.append(df)\n        print(f\"Processed {subject_id} | ECG: {len(ecg_raw)} | Labels: {len(labels)}\")\n        \n    except Exception as e:\n        print(f\"Error processing {subject_id}: {str(e)}\")\n\nif all_subjects_dfs:\n    combined_df = pd.concat(all_subjects_dfs, ignore_index=True)\n    print(f\"\\nSuccess! Combined DataFrame shape: {combined_df.shape}\")\n    print(\"Label distribution:\")\n    print(combined_df[\"label\"].value_counts())\nelse:\n    print(\"\\nNo valid data processed.\")\n\n# %% [markdown]\n# # 4. Filter & Map to Binary Stress Label\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:05.825043Z\",\"iopub.execute_input\":\"2025-05-09T21:55:05.825322Z\",\"iopub.status.idle\":\"2025-05-09T21:55:07.653137Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:05.825287Z\",\"shell.execute_reply\":\"2025-05-09T21:55:07.652367Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\n# Assuming combined_df is already loaded\nvalid_df = combined_df[combined_df[\"label\"].isin([1, 2, 3])].copy()\nvalid_df[\"binary_label\"] = np.where(valid_df[\"label\"] == 2, 1, 0)\n\n# Get the counts\nbinary_counts = valid_df[\"binary_label\"].value_counts().sort_index()\n\n# Format the output\noutput = f\"Binary Label Distribution:\\n\" + \\\n         f\"0    {binary_counts[0]:>8}  # Non-stress (baseline + amusement)\\n\" + \\\n         f\"1    {binary_counts[1]:>8}  # Stress\\n\" + \\\n         f\"Name: binary_label, dtype: int64\"\n\nprint(output)\n\n# %% [markdown]\n# # 5. Visualize Class Distribution\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:07.654049Z\",\"iopub.execute_input\":\"2025-05-09T21:55:07.654288Z\",\"iopub.status.idle\":\"2025-05-09T21:55:07.800528Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:07.654268Z\",\"shell.execute_reply\":\"2025-05-09T21:55:07.799864Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# 1. Display the first 5 rows of the DataFrame\nprint(\"Sample Data:\")\nprint(valid_df.head())\n\n# 2. Create the pie chart\nplt.figure(figsize=(8, 6))\n\n\n# Define labels and colors\nlabels = ['Non-stress (baseline + amusement)', 'Stress']\ncolors = ['#66b3ff', '#ff9999']\nexplode = (0.05, 0)  # Explode the 1st slice\n\nplt.pie(binary_counts, \n        labels=labels, \n        colors=colors,\n        autopct='%1.1f%%',\n        startangle=90,\n        explode=explode,\n        shadow=True)\n\nplt.title('Stress vs Non-Stress Distribution', fontsize=16)\nplt.axis('equal')  # Equal aspect ratio ensures pie is drawn as circle\nplt.tight_layout()\nplt.show()\n\n# %% [markdown]\n# # 6. Data segmentation functions to make windowes\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:07.801341Z\",\"iopub.execute_input\":\"2025-05-09T21:55:07.801607Z\",\"iopub.status.idle\":\"2025-05-09T21:55:16.454298Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:07.801572Z\",\"shell.execute_reply\":\"2025-05-09T21:55:16.453485Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\n\n\ndef create_labeled_windows(df, window_sec=60, sample_rate=700, min_agreement=0.75, overlap=0.5):\n    \"\"\"\n    Create labeled ECG windows from continuous data with overlap.\n    \n    Parameters:\n        df: Input DataFrame with ECG data and labels\n        window_sec: Window length in seconds\n        sample_rate: Sampling rate in Hz\n        min_agreement: Minimum agreement threshold for label assignment\n        overlap: Fractional overlap between windows (e.g., 0.5 = 50%)\n        \n    Returns:\n        windows: ECG signal windows\n        labels: Corresponding labels\n    \"\"\"\n    window_size = window_sec * sample_rate\n    step_size = int(window_size * (1 - overlap))\n    \n    windows = []\n    labels = []\n    \n    for subject, subject_df in df.groupby('subject_id'):\n        ecg_signal = subject_df['ecg_mv'].values\n        binary_labels = subject_df['binary_label'].values\n        \n        for start in range(0, len(subject_df) - window_size + 1, step_size):\n            end = start + window_size\n            window_labels = binary_labels[start:end]\n            \n            label_counts = Counter(window_labels)\n            majority_label, majority_count = label_counts.most_common(1)[0]\n            agreement = majority_count / window_size\n            \n            if agreement >= min_agreement:\n                windows.append(ecg_signal[start:end])\n                labels.append(majority_label)\n    \n    return np.array(windows), np.array(labels)\n\n# Create windows with 50% overlap\nwindows, labels = create_labeled_windows(valid_df, overlap=0.5)\n\n# Calculate statistics\ntotal_windows = len(windows)\nlabel_counts = pd.Series(labels).value_counts().sort_index()\nlabel_percent = label_counts / total_windows * 100\n\n# Text Output\nprint(f\"\\n{'='*50}\")\nprint(f\"{'WINDOW STATISTICS':^50}\")\nprint(f\"{'='*50}\")\nprint(f\"\\nTotal windows created: {total_windows:,}\")\nprint(\"\\nLabel distribution:\")\nprint(f\"Non-stress (0): {label_counts.get(0, 0):,} windows ({label_percent.get(0, 0):.1f}%)\")\nprint(f\"Stress (1):     {label_counts.get(1, 0):,} windows ({label_percent.get(1, 0):.1f}%)\")\nprint(f\"\\nClass ratio: {label_counts.get(0, 1)/label_counts.get(1, 1):.2f}:1 (Non-stress:Stress)\")\n\n# Pie Chart\nplt.subplot(1, 3, 1)\nplt.pie(label_counts, \n        labels=['Non-stress', 'Stress'],\n        colors=['#4CAF50', '#F44336'],\n        radius=2,\n        autopct=lambda p: f'{p:.1f}%\\n({int(p/100*total_windows):,})',\n        startangle=90,\n        textprops={'fontsize': 9})\nplt.title('Window Distribution', pad=40)\n\nplt.show()\n\n\n# %% [markdown]\n# # 7. Extracting Features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:16.455152Z\",\"iopub.execute_input\":\"2025-05-09T21:55:16.455392Z\",\"iopub.status.idle\":\"2025-05-09T21:55:42.538591Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:16.455372Z\",\"shell.execute_reply\":\"2025-05-09T21:55:42.537781Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\n\ndef calculate_hr(peaks, sampling_rate):\n    \"\"\"Robust heart rate calculation\"\"\"\n    if len(peaks['ECG_R_Peaks']) < 2:\n        return np.nan\n    rr_intervals = np.diff(peaks['ECG_R_Peaks']) / sampling_rate\n    return float(60 / np.mean(rr_intervals))\n\ndef extract_ecg_features(ecg_signal, sampling_rate=700):\n    \"\"\"Optimized feature extraction with guaranteed numeric output\"\"\"\n    try:\n        # 1. Preprocessing\n        cleaned = nk.ecg_clean(ecg_signal, sampling_rate=sampling_rate)\n        \n        # 2. Peak detection\n        peaks = nk.ecg_peaks(cleaned, sampling_rate=sampling_rate, method=\"kalidas2017\")[1]\n        \n        # 3. Feature extraction\n        features = {\n            # Basic ECG stats (always available)\n            'ecg_mean': float(np.mean(cleaned)),\n            'ecg_std': float(np.std(cleaned)),\n            'ecg_skew': float(pd.Series(cleaned).skew()),\n            'ecg_kurtosis': float(pd.Series(cleaned).kurtosis()),\n            \n            # Heart rate (fixed calculation)\n            'hr_mean': calculate_hr(peaks, sampling_rate),\n        }\n        \n        # 4. HRV features (only if enough peaks)\n        if len(peaks['ECG_R_Peaks']) > 4:\n            hrv_time = nk.hrv_time(peaks, sampling_rate=sampling_rate)\n            hrv_freq = nk.hrv_frequency(peaks, sampling_rate=sampling_rate)\n            \n            features.update({\n                'hrv_sdnn': float(hrv_time['HRV_SDNN'].iloc[0]),\n                'hrv_rmssd': float(hrv_time['HRV_RMSSD'].iloc[0]),\n                'hrv_pnn50': float(hrv_time['HRV_pNN50'].iloc[0]),\n                'hrv_hf': float(hrv_freq['HRV_HF'].iloc[0]),\n                'hrv_lf': float(hrv_freq['HRV_LF'].iloc[0]),\n                'hrv_lfhf': float(hrv_freq['HRV_LFHF'].iloc[0])\n            })\n        \n        return features\n        \n    except Exception as e:\n        print(f\"Error in feature extraction: {str(e)}\")\n        return None\n\ndef process_all_windows(windows, labels, sampling_rate=700):\n    \"\"\"\n    Process all ECG windows and create feature matrix\n    \n    Parameters:\n        windows (list): List of ECG window arrays\n        labels (array): Corresponding labels\n        sampling_rate (int): Sampling frequency in Hz\n        \n    Returns:\n        DataFrame: Features with labels\n    \"\"\"\n    features = []\n    for i in tqdm(range(len(windows)), desc=\"Extracting Features\"):\n        feat = extract_ecg_features(windows[i], sampling_rate)\n        if feat is not None:  # Only append if feature extraction succeeded\n            feat['label'] = labels[i]\n            features.append(feat)\n    \n    return pd.DataFrame(features).fillna(method='ffill')\n\n# Example usage (assuming you have windows and labels defined)\nif __name__ == \"__main__\":\n    # You'll need to define these variables first:\n    # windows = [your_ecg_data_arrays]\n    # labels = [corresponding_labels]\n    \n    feature_df = process_all_windows(windows, labels)\n\n    # Display final results\n    print(\"\\n\" + \"=\"*50)\n    print(f\"{'FINAL FEATURE EXTRACTION RESULTS':^50}\")\n    print(\"=\"*50)\n    print(f\"\\nTotal windows processed: {len(feature_df)}\")\n    print(f\"Success rate: {100*(1-feature_df.isna().mean().mean()):.1f}%\")\n    print(\"\\nLabel distribution:\")\n    print(feature_df['label'].value_counts())\n    print(\"\\nFeature preview (no NaN values):\")\n    print(feature_df.dropna().head(3).to_string(float_format=\"%.3f\"))\n\n# %% [markdown]\n# # 8. New Dataframe after Extracting Features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:42.541156Z\",\"iopub.execute_input\":\"2025-05-09T21:55:42.541393Z\",\"iopub.status.idle\":\"2025-05-09T21:55:42.549313Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:42.541373Z\",\"shell.execute_reply\":\"2025-05-09T21:55:42.548516Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\n# Display first 5 rows with formatted floats\npd.set_option('display.float_format', '{:.3f}'.format)\ndisplay(feature_df.head().style.set_caption(\"First 5 Windows with Extracted Features\"))\n\n# %% [markdown]\n# # 9. Feature-Label Correlation Analysis first step in feature selection\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:42.550501Z\",\"iopub.execute_input\":\"2025-05-09T21:55:42.550777Z\",\"iopub.status.idle\":\"2025-05-09T21:55:42.565430Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:42.550733Z\",\"shell.execute_reply\":\"2025-05-09T21:55:42.564735Z\"}}\ncorrelation_matrix = feature_df.corr()\nlabel_correlation = correlation_matrix['label'].abs().sort_values(ascending=False)\nprint(label_correlation)\n\n# %% [markdown]\n# # 10. Feature selection using correlation threshold\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T21:55:42.566206Z\",\"iopub.execute_input\":\"2025-05-09T21:55:42.566411Z\",\"iopub.status.idle\":\"2025-05-09T21:55:42.581275Z\",\"shell.execute_reply.started\":\"2025-05-09T21:55:42.566392Z\",\"shell.execute_reply\":\"2025-05-09T21:55:42.580545Z\"}}\n# Automatically drop features with correlation < threshold\nlow_corr_threshold = 0.1\nlow_corr_features = label_correlation[label_correlation < low_corr_threshold].index.tolist()\nfeature_df_optimized = feature_df.drop(columns=low_corr_features)\n# Display first 5 rows with formatted floats\npd.set_option('display.float_format', '{:.3f}'.format)\ndisplay(feature_df_optimized.head().style.set_caption(\"First 5 Windows with Extracted Features\"))\n\n# %% [markdown]\n# # 11. Train/Test Split of data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:17:08.655474Z\",\"iopub.execute_input\":\"2025-05-09T22:17:08.655805Z\",\"iopub.status.idle\":\"2025-05-09T22:17:08.664740Z\",\"shell.execute_reply.started\":\"2025-05-09T22:17:08.655784Z\",\"shell.execute_reply\":\"2025-05-09T22:17:08.663920Z\"}}\n\nfrom sklearn.model_selection import train_test_split\n\n# Standard stratified split (single line)\nX_train, X_test, y_train, y_test = train_test_split(\n    feature_df_optimized.drop(columns=['label']),\n    feature_df_optimized['label'],\n    test_size=0.3,\n    random_state=42,\n    stratify=feature_df_optimized['label']  # Automatic balancing\n)\n\n# %% [markdown]\n# # 12. Apply SMOTE to balnce data \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:17:08.684613Z\",\"iopub.execute_input\":\"2025-05-09T22:17:08.684831Z\",\"iopub.status.idle\":\"2025-05-09T22:17:08.697382Z\",\"shell.execute_reply.started\":\"2025-05-09T22:17:08.684805Z\",\"shell.execute_reply\":\"2025-05-09T22:17:08.696641Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# Apply SMOTE with 1:1 ratio\nsmote = SMOTE(\n    sampling_strategy=1.0,  # Force exact balance (263:263)\n    k_neighbors=5,          # Optimal for ECG feature space\n    random_state=42\n)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Verify\nprint(\"\\nAfter SMOTE:\")\nprint(pd.Series(y_train_smote).value_counts())\nprint(f\"New ratio: {y_train_smote.mean():.1%}\")\n\n# %% [markdown]\n# # 13. ðŸ“Š Model Comparison Pipeline\n# # Tests multiple classifiers to identify the best performing model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:24:22.839490Z\",\"iopub.execute_input\":\"2025-05-09T22:24:22.839845Z\",\"iopub.status.idle\":\"2025-05-09T22:24:23.463910Z\",\"shell.execute_reply.started\":\"2025-05-09T22:24:22.839798Z\",\"shell.execute_reply\":\"2025-05-09T22:24:23.463222Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\nmodel = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=11,\n \n    random_state=42\n)\nmodel.fit(X_train_smote, y_train_smote)\n#model.fit(X_train, y_train)\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.2%}\")\nprint(\"\\nDetailed Report:\")\nprint(classification_report(y_test, y_pred, target_names=[\"Non-stress\", \"Stress\"]))\n\n# 3. Confusion Matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, \n                                      display_labels=[\"Non-stress\", \"Stress\"],\n                                      cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:17:09.834843Z\",\"iopub.execute_input\":\"2025-05-09T22:17:09.835094Z\",\"iopub.status.idle\":\"2025-05-09T22:17:09.845660Z\",\"shell.execute_reply.started\":\"2025-05-09T22:17:09.835073Z\",\"shell.execute_reply\":\"2025-05-09T22:17:09.844831Z\"}}\n\n# Fit scaler ONLY on training data\nscaler = RobustScaler().fit(X_train)  # Use SMOTE-augmented data\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Transform test set with same scaler\n\n# Verify no data leakage\nprint(f\"Test set scaled (sample):\\n{X_test_scaled[0,:5].round(2)}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:17:09.846295Z\",\"iopub.execute_input\":\"2025-05-09T22:17:09.846551Z\",\"iopub.status.idle\":\"2025-05-09T22:17:09.937024Z\",\"shell.execute_reply.started\":\"2025-05-09T22:17:09.846529Z\",\"shell.execute_reply\":\"2025-05-09T22:17:09.936171Z\"}}\n\n# Initialize and train the SVM model\nsvm_model = SVC(kernel='rbf', C=36, gamma='scale', probability=True, random_state=42)  # Adjust C, gamma if needed\nsvm_model.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred_svm = svm_model.predict(X_test_scaled)\n\n# Evaluate the model\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nprint(f\"SVM Model Accuracy: {accuracy_svm:.2%}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred_svm, target_names=[\"Non-stress\", \"Stress\"]))\n\n# Calculate AUC\nroc_auc_svm = roc_auc_score(y_test, svm_model.predict_proba(X_test_scaled)[:, 1])\nprint(f\"AUC: {roc_auc_svm:.2%}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:26:05.299011Z\",\"iopub.execute_input\":\"2025-05-09T22:26:05.299341Z\",\"iopub.status.idle\":\"2025-05-09T22:26:05.628803Z\",\"shell.execute_reply.started\":\"2025-05-09T22:26:05.299316Z\",\"shell.execute_reply\":\"2025-05-09T22:26:05.628116Z\"}}\n\n\n# 1. Initialize and train XGBoost with balanced classes\nxgb_model = XGBClassifier(\n    max_depth=5,               # Control tree depth\n    learning_rate=0.1,         # Shrinkage to prevent overfitting\n    n_estimators=300,          # Number of boosting rounds\n    scale_pos_weight=2.375,    # Counter class imbalance (48 stress / 114 non-stress â‰ˆ 2.375)\n    objective='binary:logistic',\n    eval_metric='logloss',     # Alternative: 'auc'\n    random_state=42\n)\n\n# Train the model with SMOTE-augmented data\nxgb_model.fit(X_train_smote, y_train_smote)\n\n# 2. Evaluate the model\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Accuracy\naccuracy_xgb = accuracy_score(y_test, y_pred_xgb)\nprint(f\"XGBoost Model Accuracy: {accuracy_xgb:.2%}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test, y_pred_xgb, target_names=[\"Non-stress\", \"Stress\"]))\n\n# 3. Confusion Matrix\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb, \n                                         display_labels=[\"Non-stress\", \"Stress\"],\n                                         cmap='Blues')\nplt.title(\"XGBoost Confusion Matrix\")\nplt.show()\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:26:37.456640Z\",\"iopub.execute_input\":\"2025-05-09T22:26:37.456952Z\",\"iopub.status.idle\":\"2025-05-09T22:26:37.641351Z\",\"shell.execute_reply.started\":\"2025-05-09T22:26:37.456927Z\",\"shell.execute_reply\":\"2025-05-09T22:26:37.640651Z\"}}\n\n# 1. Initialize and train Decision Tree\ndt_model = DecisionTreeClassifier(\n    max_depth=20,                \n    criterion='entropy',        # Use entropy for information gain\n    random_state=42\n)\ndt_model.fit(X_train_smote, y_train_smote)\n# 2. Evaluate\ny_pred = dt_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Decision Tree Accuracy: {accuracy:.2%}\")\nprint(\"\\nDetailed Report:\")\nprint(classification_report(y_test, y_pred, target_names=[\"Non-stress\", \"Stress\"]))\n\n# 3. Confusion Matrix\nConfusionMatrixDisplay.from_predictions(\n    y_test, y_pred,\n    display_labels=[\"Non-stress\", \"Stress\"],\n    cmap='Blues'\n)\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.show()\n\n# %% [markdown]\n# # 14. ROC curve to show how good model to discriminate between two clases\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-09T22:17:09.937914Z\",\"iopub.execute_input\":\"2025-05-09T22:17:09.938237Z\",\"iopub.status.idle\":\"2025-05-09T22:17:10.134456Z\",\"shell.execute_reply.started\":\"2025-05-09T22:17:09.938209Z\",\"shell.execute_reply\":\"2025-05-09T22:17:10.133602Z\"}}\n\n\n# 1. Predict probabilities for the positive class (Stress = 1)\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# 2. Compute ROC curve and ROC area\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# 3. Plot the ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n","metadata":{"_uuid":"faa89bbd-a810-41f8-be5e-a75fb397f387","_cell_guid":"88bb1730-ff5d-4f27-a0c4-910c3418c6e6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}